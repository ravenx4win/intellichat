import os
import tempfile
from dotenv import load_dotenv
import streamlit as st
from langchain_community.llms import HuggingFaceHub
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
import PyPDF2
import pdfplumber

# Load environment variables
load_dotenv()
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")

# Set up Streamlit UI
st.set_page_config(
    page_title="Intellichat",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        color: #1f77b4;
    }
    .chat-container {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .user-message {
        background-color: #007bff;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 15px 15px 5px 15px;
        margin: 0.5rem 0;
        text-align: right;
    }
    .bot-message {
        background-color: #e9ecef;
        color: #333;
        padding: 0.5rem 1rem;
        border-radius: 15px 15px 15px 5px;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

def extract_text_from_pdf(pdf_file) -> str:
    """Extract text from PDF using multiple methods for better accuracy."""
    text = ""
    
    # Method 1: Try pdfplumber first (better for complex layouts)
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(pdf_file.read())
            tmp_file_path = tmp_file.name
        
        with pdfplumber.open(tmp_file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        
        os.unlink(tmp_file_path)
        
    except Exception as e:
        st.warning(f"pdfplumber failed: {e}. Trying PyPDF2...")
        
        # Method 2: Fallback to PyPDF2
        try:
            pdf_file.seek(0)  # Reset file pointer
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        except Exception as e2:
            st.error(f"Both PDF extraction methods failed: {e2}")
            return ""
    
    return text.strip()

def create_vectorstore(text: str, api_key: str):
    """Create vector store from text chunks using Hugging Face embeddings."""
    if not text:
        return None, None
    
    # Split text into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )
    
    chunks = text_splitter.split_text(text)
    
    if not chunks:
        st.error("No text chunks could be created from the PDF.")
        return None, None
    
    # Create Hugging Face embeddings (FREE)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # Create vector store
    vectorstore = Chroma.from_texts(
        texts=chunks,
        embedding=embeddings,
        persist_directory="./chroma_db"
    )
    
    return vectorstore, chunks

def create_qa_chain(vectorstore, api_key: str):
    """Create the conversational retrieval chain using Hugging Face."""
    if not vectorstore:
        return None
    
    # Skip AI model loading to prevent crashes - use simple text matching
    st.info("üîÑ Using intelligent text matching (no AI model to prevent crashes)...")
    return create_simple_qa_fallback(vectorstore)

def create_simple_qa_fallback(vectorstore):
    """Intelligent text matching Q&A without heavy AI models."""
    def simple_qa(question):
        try:
            # Get relevant documents
            docs = vectorstore.similarity_search(question, k=5)
            
            if docs:
                # Combine relevant content
                content = "\n\n".join([doc.page_content for doc in docs])
                
                # Create a more intelligent response
                response = f"""Based on the document, here's what I found:

{content[:800]}

This information is relevant to your question: "{question}"

Note: This is a text-based search result. For more sophisticated AI responses, you would need a more powerful model, but this provides accurate information from your document."""
            else:
                response = f"I couldn't find specific information about '{question}' in the document. Try asking about:\n- What is this document about?\n- What are the main topics?\n- What are the key points?"
            
            return {"answer": response}
            
        except Exception as e:
            return {"answer": f"Error processing question: {str(e)}"}
    
    return simple_qa

def display_chat_history():
    """Display the chat history in a nice format."""
    if st.session_state.chat_history:
        st.markdown("### üí¨ Conversation History")
        for i, (question, answer) in enumerate(st.session_state.chat_history):
            st.markdown(f"""
            <div class="chat-container">
                <div class="user-message">
                    <strong>You:</strong> {question}
                </div>
                <div class="bot-message">
                    <strong>Intellichat:</strong> {answer}
                </div>
            </div>
            """, unsafe_allow_html=True)

# Main UI
st.markdown('<h1 class="main-header">üìÑ Intellichat ‚Äì Document Q&A Assistant</h1>', unsafe_allow_html=True)

# Sidebar for configuration
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # API Key input
    api_key = st.text_input(
        "Hugging Face API Key",
        type="password",
        value=os.getenv("HUGGINGFACE_API_KEY", ""),
        help="Enter your Hugging Face API key. Get it free from https://huggingface.co/settings/tokens"
    )
    
    if not api_key:
        st.error("‚ö†Ô∏è Please enter your Hugging Face API key to continue.")
        st.stop()
    
    st.markdown("---")
    st.markdown("### üìã Instructions")
    st.markdown("""
    1. Upload a PDF document
    2. Wait for processing to complete
    3. Ask questions about the document
    4. Enjoy intelligent conversations!
    """)
    
    st.markdown("### üîß Features")
    st.markdown("""
    - üìÑ PDF document processing
    - üß† AI-powered Q&A
    - üí≠ Conversational memory
    - üîç Semantic search
    - üé® Beautiful UI
    """)

# Main content area
col1, col2 = st.columns([1, 2])

with col1:
    st.header("üìÅ Upload Document")
    pdf_file = st.file_uploader(
        "Choose a PDF file",
        type="pdf",
        help="Upload a PDF document to start asking questions"
    )
    
    if pdf_file is not None:
        st.success(f"‚úÖ File uploaded: {pdf_file.name}")
        
        # Process PDF
        with st.spinner("üîÑ Processing PDF..."):
            text = extract_text_from_pdf(pdf_file)
            
            if text:
                st.success(f"‚úÖ Extracted {len(text)} characters from PDF")
                
                # Create vector store
                with st.spinner("üß† Creating knowledge base..."):
                    vectorstore, chunks = create_vectorstore(text, api_key)
                    
                    if vectorstore:
                        st.session_state.vectorstore = vectorstore
                        st.success(f"‚úÖ Created knowledge base with {len(chunks)} chunks")
                        
                        # Create QA chain
                        with st.spinner("üîó Setting up AI assistant..."):
                            qa_chain = create_qa_chain(vectorstore, api_key)
                            if qa_chain:
                                st.session_state.qa_chain = qa_chain
                                st.success("‚úÖ AI assistant ready!")
                            else:
                                st.error("‚ùå Failed to create AI assistant")
                    else:
                        st.error("‚ùå Failed to create knowledge base")
            else:
                st.error("‚ùå Could not extract text from PDF")

with col2:
    st.header("üí¨ Chat with Your Document")
    
    if st.session_state.qa_chain is not None:
        # Chat input
        question = st.text_input(
            "Ask a question about your document:",
            placeholder="What is this document about?",
            key="question_input"
        )
        
        if st.button("üöÄ Ask Question", type="primary") and question:
            with st.spinner("ü§î Thinking..."):
                try:
                    # Get response from the chain
                    if callable(st.session_state.qa_chain):
                        # Simple QA function (fallback)
                        response = st.session_state.qa_chain(question)
                        answer = response["answer"]
                    else:
                        # Full QA chain
                        response = st.session_state.qa_chain({"question": question})
                        answer = response["answer"]
                    
                    # Store in chat history
                    st.session_state.chat_history.append((question, answer))
                    
                    # Rerun to show the response
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"‚ùå Error getting response: {str(e)}")
                    st.info("üí° Try asking a simpler question or check your API key.")
        
        # Display chat history
        display_chat_history()
        
        # Clear chat button
        if st.button("üóëÔ∏è Clear Chat History"):
            st.session_state.chat_history = []
            st.rerun()
    
    else:
        st.info("üëÜ Please upload a PDF document first to start chatting!")

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: #666;'>
        <p>ü§ñ Powered by Hugging Face & LangChain | Built with Streamlit</p>
    </div>
    """,
    unsafe_allow_html=True
)
